# Topic 4 Exercises: Classification
##Homework 4

### Nadia Berriel

#Programming Assignment: 4.7.11, 4.7.13

##Problem 4.7.11

#Part A
```{r}
library(MASS)
library(ISLR)
Auto$mpg01 <- ifelse(Auto$mpg < mean(Auto$mpg),1,0)
```

#Part B
```{r}
pairs(Auto[ ,-9])
```

#Part C
```{r}
set.seed(1)
rands <- rnorm(nrow(Auto))
test <- rands > quantile(rands,0.75)
train <- !test
Auto.train <- Auto[train,]
Auto.test <- Auto[test,]
```

#Part D
```{r}
lda.fit = lda(mpg01 ~ horsepower+weight+acceleration, data=Auto.train)
lda.fit
lda.class=predict(lda.fit,Auto.test)$class
table(lda.class,Auto.test$mpg01)
mean(lda.class==Auto.test$mpg01)
```
LDA achieved 88.8% test accuracy.

#Part E
```{r}
qda.fit = qda(mpg01 ~ horsepower+weight+acceleration, data=Auto.train)
qda.fit
qda.class=predict(qda.fit,Auto.test)$class
table(qda.class,Auto.test$mpg01)
mean(qda.class==Auto.test$mpg01)
```
QDA performed a little better, and achieved 92.9% accuracy on the test set.

#Part F
```{r}
logit.fit = glm(mpg01 ~ horsepower+weight+acceleration, family=binomial, data=Auto.train)
summary(logit.fit)
glm.probs=predict(logit.fit,Auto.test,type="response")
glm.pred=rep(0,nrow(Auto.test))
glm.pred[glm.probs > 0.50]=1
table(glm.pred,Auto.test$mpg01)
mean(glm.pred==Auto.test$mpg01)
```

#PArt G
```{r}
set.seed(1)
train.Auto = Auto.train[,c("horsepower","weight","acceleration")]
test.Auto =  Auto.test[,c("horsepower","weight","acceleration")]
knn.pred=knn(train.Auto,test.Auto,Auto.train$mpg01,k=1)
table(knn.pred,Auto.test$mpg01)
mean(knn.pred==Auto.test$mpg01)

knn.pred=knn(train.Auto,test.Auto,Auto.train$mpg01,k=2)
table(knn.pred,Auto.test$mpg01)
mean(knn.pred==Auto.test$mpg01)

knn.pred=knn(train.Auto,test.Auto,Auto.train$mpg01,k=3)
table(knn.pred,Auto.test$mpg01)
mean(knn.pred==Auto.test$mpg01)

knn.pred=knn(train.Auto,test.Auto,Auto.train$mpg01,k=4)
table(knn.pred,Auto.test$mpg01)
mean(knn.pred==Auto.test$mpg01)

knn.pred=knn(train.Auto,test.Auto,Auto.train$mpg01,k=5)
table(knn.pred,Auto.test$mpg01)
mean(knn.pred==Auto.test$mpg01)

knn.pred=knn(train.Auto,test.Auto,Auto.train$mpg01,k=11)
table(knn.pred,Auto.test$mpg01)
mean(knn.pred==Auto.test$mpg01)



```


##Problem 4.7.13
```{r}
Boston$crim01 <- as.numeric(Boston$crim > median(Boston$crim))

set.seed(1)
rands <- rnorm(nrow(Boston))
test <- rands > quantile(rands,0.75)
train <- !test
Boston.train <- Boston[train,]
Boston.test <- Boston[test,]

Boston.train.fact <- Boston.train
Boston.train.fact$crim01 <- factor(Boston.train.fact$crim01)
library(GGally)
ggpairs(Boston.train.fact, colour='crim01')


glm.fit=glm(crim01~lstat+rm+zn+nox+dis+rad+ptratio+black+medv+age+chas+indus+tax, data=Boston.train)
summary(glm.fit)

glm.probs=predict(glm.fit,Boston.test,type="response")
glm.pred=rep(0,nrow(Boston.test))
glm.pred[glm.probs > 0.50]=1
table(glm.pred,Boston.test$crim01)
mean(glm.pred==Boston.test$crim01)

glm.fit=glm(crim01~nox+rad+medv+age+tax, data=Boston.train)
summary(glm.fit)

glm.probs=predict(glm.fit,Boston.test,type="response")
glm.pred=rep(0,nrow(Boston.test))
glm.pred[glm.probs > 0.50]=1
table(glm.pred,Boston.test$crim01)
mean(glm.pred==Boston.test$crim01)


glm.fit=glm(crim01~nox*dis+medv:tax+rad+age, data=Boston.train)
summary(glm.fit)

glm.probs=predict(glm.fit,Boston.test,type="response")
glm.pred=rep(0,nrow(Boston.test))
glm.pred[glm.probs > 0.50]=1
table(glm.pred,Boston.test$crim01)
mean(glm.pred==Boston.test$crim01)

glm.fit=glm(crim01~nox+rad+medv+age+tax+ptratio+indus, data=Boston.train)
summary(glm.fit)
#NOX,RAD,MEDV,AGE,TAX look good
glm.probs=predict(glm.fit,Boston.test,type="response")
glm.pred=rep(0,nrow(Boston.test))
glm.pred[glm.probs > 0.50]=1
table(glm.pred,Boston.test$crim01)
mean(glm.pred==Boston.test$crim01)

glm.fit=glm(crim01~nox+rad+medv+age+tax+indus, data=Boston.train)
summary(glm.fit)

glm.probs=predict(glm.fit,Boston.test,type="response")
glm.pred=rep(0,nrow(Boston.test))
glm.pred[glm.probs > 0.50]=1
table(glm.pred,Boston.test$crim01)
mean(glm.pred==Boston.test$crim01)


########################
# LDA
lda.fit=lda(crim01~nox+rad+medv+age+tax+ptratio, data=Boston.train)
lda.fit
#NOX,RAD,MEDV,AGE,TAX look good, ptratio seems to help also
lda.pred=predict(lda.fit,Boston.test)$class
table(lda.pred,Boston.test$crim01)
mean(lda.pred==Boston.test$crim01)

########################
# KNN
set.seed(1)
train.Boston = Boston.train[,c("nox","rad","medv","age","tax","ptratio")]
test.Boston =  Boston.test[,c("nox","rad","medv","age","tax","ptratio")]
knn.pred=knn(train.Boston,test.Boston,Boston.train$crim01,k=1)
table(knn.pred,Boston.test$crim01)
mean(knn.pred==Boston.test$crim01)

knn.pred=knn(train.Boston,test.Boston,Boston.train$crim01,k=2)
table(knn.pred,Boston.test$crim01)
mean(knn.pred==Boston.test$crim01)

knn.pred=knn(train.Boston,test.Boston,Boston.train$crim01,k=3)
table(knn.pred,Boston.test$crim01)
mean(knn.pred==Boston.test$crim01)

knn.pred=knn(train.Boston,test.Boston,Boston.train$crim01,k=4)
table(knn.pred,Boston.test$crim01)
mean(knn.pred==Boston.test$crim01)

knn.pred=knn(train.Boston,test.Boston,Boston.train$crim01,k=5)
table(knn.pred,Boston.test$crim01)
mean(knn.pred==Boston.test$crim01)


knn.pred=knn(train.Boston,test.Boston,Boston.train$crim01,k=11)
table(knn.pred,Boston.test$crim01)
mean(knn.pred==Boston.test$crim01)

```


The LDA/logistic regression was 89%. With the KNN, I got a model that got up to 92%. K=1 got to 93%, but K=3 was nearly as good, and the higher K might be more robust going forward.


###Theory assignment: 4.7.1, 4.7.8, 4.7.9

#Problem 4.7.1
1−p(X) = 1−e^β_0+β_1X_1+e_β_0+β_1X=11+e^β_0+β_1X,

1/(1−p(X)) = 1+e^β_0+β_1X

p(X) * 1/(1−p(X)) = (eβ_0+β_1X)/(1+e^β_0+β_1X) * (1+eβ0+β1X))

p(X)/(1−p(X)) = e^β_0+β_1X


#Problem 4.7.8

This is due to the logistic regression. When K=1 for KNN approach, the training error is zero, therefore the test error for KNN was 36%. It was higher than logistic test error.

#Problem 4.7.9

#Part A
```{r}
print(0.37/(1+0.37))
```

#Part B
```{r}
print(odds <- .16/(1-.16))
```
