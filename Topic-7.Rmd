# Topic 7 Exercises: Nonlinear regression

###NADIA BERRIEL


## 7.9.11
a)-d)
```{r}
set.seed(6)
x1 <- rnorm(100)
x2 <- rnorm(100)
y <- 6 + 10*x1 + 0.5*x2 + rnorm(100)
beta1 <- 0.1
a <- y - beta1*x1
beta2 <- lm(a~x2)$coef[2]
a <- y-beta2*x2
beta1 <- lm(a~x1)$coef[2]
```

e)
```{r}
beta_0 <- numeric(1000)
beta_1 <- numeric(1000)
beta_2 <- numeric(1000)
beta1 <- 0.1
for (i in 1:1000){
  a <- y-beta1*x1
  beta2 <- lm(a~x2)$coef[2]
  a <- y-beta2*x2
  beta1 <- lm(a~x1)$coef[2]
  beta0 <- lm(a~x1)$coef[1]
  beta_0[i] <- beta0
  beta_1[i] <- beta1
  beta_2[i] <- beta2
}
plot(beta_0, type="l",xlab="iteration",ylab="value", ylim = c(0,12), col="red")
lines(beta_1, col="green", type="l")
lines(beta_2, col="blue", type = "l")
legend("bottomright", c("beta0","beta1","beta2"),lty = 1, col=c("red","green","blue"))
```

f)
```{r}
mlg <- lm(y~x1+x2)
plot(beta_0, type="l",xlab="iteration",ylab="value", ylim = c(0,12), col="red")
lines(beta_1, col="green", type="l")
lines(beta_2, col="blue", type = "l")
abline(h=mlg$coef[1], lty="dashed", lwd=3, col=rgb(0,0,0, alpha=1/3))
abline(h=mlg$coef[2], lty="dashed", lwd=3, col=rgb(0,0,0, alpha=1/3))
abline(h=mlg$coef[3], lty="dashed",lwd=3, col=rgb(0,0,0, alpha=1/3))
legend("right", c("beta0","beta1","beta2","multiple regression"),lty = c(1,1,1,2), col=c("red","green","blue","black"))
```

g) Only one iteration was required

## 7.9.3
The intercept is at (0,1). From -2 to 1, the graph is a line, slope 1, going through (0,1). From 1 to 2, the graph is a curve concaving down, starting at (1,2) and ending at (2,1).

## 7.9.4
The graph is discontinuous and different at these different interval:
[-2,0): straight line at y=1
[0,1): straight line at y=2
[1,2]: line intercept at (1,2), slope -1

## 7.9.5
a) g2 will have smaller train RSS because g2 is more flexible (degree 4+ polynomial) when lambda goes to infinity, the penalty term is the same for both model.
b) g1 will likely to have smaller test RSS because g2 is so flexible, g2 is more prompt to overfit the data and produce higher test RSS
c) g2 will have smaller training RSS and larger testing RSS for similar reasons as above when the penalty term is omitted (lambda = 0)